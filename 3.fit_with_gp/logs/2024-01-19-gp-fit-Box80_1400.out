Leaving out Box80_1400
Box47_1400 2.3038301302700224
{'ns': 0.97000003, 'H0': 67.0, 'w0': -1.0, 'ombh2': 0.0223, 'omch2': 0.12, 'nu_mass_ev': 0.07071068, '10^9 As': 2.10100315}
(2234, 8)
(2234, 4)
scaling input
(2234, 8)
scaling output
(2234, 4)
Iter 1/200 - Loss: 2.3743
Iter 2/200 - Loss: 2.0982
Iter 3/200 - Loss: 1.8427
Iter 4/200 - Loss: 1.6078
Iter 5/200 - Loss: 1.3938
Iter 6/200 - Loss: 1.2012
Iter 7/200 - Loss: 1.0306
Iter 8/200 - Loss: 0.8837
Iter 9/200 - Loss: 0.7634
Iter 10/200 - Loss: 0.6728
Iter 11/200 - Loss: 0.6131
Iter 12/200 - Loss: 0.5823
Iter 13/200 - Loss: 0.5718
Iter 14/200 - Loss: 0.5679
Iter 15/200 - Loss: 0.5586
Iter 16/200 - Loss: 0.5374
Iter 17/200 - Loss: 0.5024
Iter 18/200 - Loss: 0.4542
Iter 19/200 - Loss: 0.3964
Iter 20/200 - Loss: 0.3337
Iter 21/200 - Loss: 0.2704
Iter 22/200 - Loss: 0.2099
Iter 23/200 - Loss: 0.1552
Iter 24/200 - Loss: 0.1086
Iter 25/200 - Loss: 0.0705
Iter 26/200 - Loss: 0.0383
Iter 27/200 - Loss: 0.0087
Iter 28/200 - Loss: -0.0209
Iter 29/200 - Loss: -0.0522
Iter 30/200 - Loss: -0.0859
Iter 31/200 - Loss: -0.1219
Iter 32/200 - Loss: -0.1596
Iter 33/200 - Loss: -0.1978
Iter 34/200 - Loss: -0.2350
Iter 35/200 - Loss: -0.2692
Iter 36/200 - Loss: -0.2988
Iter 37/200 - Loss: -0.3237
Iter 38/200 - Loss: -0.3455
Iter 39/200 - Loss: -0.3674
Iter 40/200 - Loss: -0.3914
Iter 41/200 - Loss: -0.4169
Iter 42/200 - Loss: -0.4419
Iter 43/200 - Loss: -0.4649
Iter 44/200 - Loss: -0.4848
Iter 45/200 - Loss: -0.5022
Iter 46/200 - Loss: -0.5177
Iter 47/200 - Loss: -0.5322
Iter 48/200 - Loss: -0.5460
Iter 49/200 - Loss: -0.5596
Iter 50/200 - Loss: -0.5731
Iter 51/200 - Loss: -0.5862
Iter 52/200 - Loss: -0.5987
Iter 53/200 - Loss: -0.6104
Iter 54/200 - Loss: -0.6214
Iter 55/200 - Loss: -0.6314
Iter 56/200 - Loss: -0.6408
Iter 57/200 - Loss: -0.6502
Iter 58/200 - Loss: -0.6593
Iter 59/200 - Loss: -0.6683
Iter 60/200 - Loss: -0.6767
Iter 61/200 - Loss: -0.6845
Iter 62/200 - Loss: -0.6916
Iter 63/200 - Loss: -0.6978
Iter 64/200 - Loss: -0.7037
Iter 65/200 - Loss: -0.7094
Iter 66/200 - Loss: -0.7148
Iter 67/200 - Loss: -0.7198
Iter 68/200 - Loss: -0.7244
Iter 69/200 - Loss: -0.7283
Iter 70/200 - Loss: -0.7317
Iter 71/200 - Loss: -0.7346
Iter 72/200 - Loss: -0.7373
Iter 73/200 - Loss: -0.7399
Iter 74/200 - Loss: -0.7422
Iter 75/200 - Loss: -0.7441
Iter 76/200 - Loss: -0.7458
Iter 77/200 - Loss: -0.7474
Iter 78/200 - Loss: -0.7490
Iter 79/200 - Loss: -0.7505
Iter 80/200 - Loss: -0.7521
Iter 81/200 - Loss: -0.7535
Iter 82/200 - Loss: -0.7548
Iter 83/200 - Loss: -0.7558
Iter 84/200 - Loss: -0.7568
Iter 85/200 - Loss: -0.7575
Iter 86/200 - Loss: -0.7583
Iter 87/200 - Loss: -0.7590
Iter 88/200 - Loss: -0.7597
Iter 89/200 - Loss: -0.7606
Iter 90/200 - Loss: -0.7616
Iter 91/200 - Loss: -0.7635
Iter 92/200 - Loss: -0.7662
Iter 93/200 - Loss: -0.7702
Iter 94/200 - Loss: -0.7761
Iter 95/200 - Loss: -0.7836
Iter 96/200 - Loss: -0.7921
Iter 97/200 - Loss: -0.8004
Iter 98/200 - Loss: -0.8078
Iter 99/200 - Loss: -0.8142
Iter 100/200 - Loss: -0.8199
Iter 101/200 - Loss: -0.8252
Iter 102/200 - Loss: -0.8305
Iter 103/200 - Loss: -0.8359
Iter 104/200 - Loss: -0.8414
Iter 105/200 - Loss: -0.8468
Iter 106/200 - Loss: -0.8518
Iter 107/200 - Loss: -0.8562
Iter 108/200 - Loss: -0.8599
Iter 109/200 - Loss: -0.8630
Iter 110/200 - Loss: -0.8656
Iter 111/200 - Loss: -0.8680
Iter 112/200 - Loss: -0.8702
Iter 113/200 - Loss: -0.8724
Iter 114/200 - Loss: -0.8743
Iter 115/200 - Loss: -0.8761
Iter 116/200 - Loss: -0.8776
Iter 117/200 - Loss: -0.8789
Iter 118/200 - Loss: -0.8800
Iter 119/200 - Loss: -0.8809
Iter 120/200 - Loss: -0.8818
Iter 121/200 - Loss: -0.8827
Iter 122/200 - Loss: -0.8836
Iter 123/200 - Loss: -0.8846
Iter 124/200 - Loss: -0.8857
Iter 125/200 - Loss: -0.8868
Iter 126/200 - Loss: -0.8879
Iter 127/200 - Loss: -0.8890
Iter 128/200 - Loss: -0.8902
Iter 129/200 - Loss: -0.8914
Iter 130/200 - Loss: -0.8926
Iter 131/200 - Loss: -0.8937
Iter 132/200 - Loss: -0.8950
Iter 133/200 - Loss: -0.8962
Iter 134/200 - Loss: -0.8974
Iter 135/200 - Loss: -0.8986
Iter 136/200 - Loss: -0.8996
Iter 137/200 - Loss: -0.9007
Iter 138/200 - Loss: -0.9018
Iter 139/200 - Loss: -0.9027
Iter 140/200 - Loss: -0.9035
Iter 141/200 - Loss: -0.9044
Iter 142/200 - Loss: -0.9052
Iter 143/200 - Loss: -0.9057
Iter 144/200 - Loss: -0.9064
Iter 145/200 - Loss: -0.9071
Iter 146/200 - Loss: -0.9077
Iter 147/200 - Loss: -0.9084
Iter 148/200 - Loss: -0.9091
Iter 149/200 - Loss: -0.9098
Iter 150/200 - Loss: -0.9104
Iter 151/200 - Loss: -0.9109
reducing lr to 0.01
Iter 152/200 - Loss: -0.9116
Iter 153/200 - Loss: -0.9117
Iter 154/200 - Loss: -0.9117
Iter 155/200 - Loss: -0.9118
Iter 156/200 - Loss: -0.9118
Iter 157/200 - Loss: -0.9120
Iter 158/200 - Loss: -0.9120
Iter 159/200 - Loss: -0.9120
Iter 160/200 - Loss: -0.9121
Iter 161/200 - Loss: -0.9122
Iter 162/200 - Loss: -0.9123
Iter 163/200 - Loss: -0.9124
Iter 164/200 - Loss: -0.9123
Iter 165/200 - Loss: -0.9125
Iter 166/200 - Loss: -0.9125
Iter 167/200 - Loss: -0.9126
Iter 168/200 - Loss: -0.9127
Iter 169/200 - Loss: -0.9128
Iter 170/200 - Loss: -0.9128
Iter 171/200 - Loss: -0.9129
Iter 172/200 - Loss: -0.9129
Iter 173/200 - Loss: -0.9130
Iter 174/200 - Loss: -0.9130
Iter 175/200 - Loss: -0.9132
Iter 176/200 - Loss: -0.9132
Iter 177/200 - Loss: -0.9133
Iter 178/200 - Loss: -0.9134
Iter 179/200 - Loss: -0.9134
Iter 180/200 - Loss: -0.9135
Iter 181/200 - Loss: -0.9135
Iter 182/200 - Loss: -0.9136
Iter 183/200 - Loss: -0.9137
Iter 184/200 - Loss: -0.9137
Iter 185/200 - Loss: -0.9139
Iter 186/200 - Loss: -0.9139
Iter 187/200 - Loss: -0.9140
Iter 188/200 - Loss: -0.9140
Iter 189/200 - Loss: -0.9140
Iter 190/200 - Loss: -0.9141
Iter 191/200 - Loss: -0.9142
Iter 192/200 - Loss: -0.9142
Iter 193/200 - Loss: -0.9143
Iter 194/200 - Loss: -0.9144
Iter 195/200 - Loss: -0.9145
Iter 196/200 - Loss: -0.9145
Iter 197/200 - Loss: -0.9146
Iter 198/200 - Loss: -0.9147
Iter 199/200 - Loss: -0.9148
Iter 200/200 - Loss: -0.9148
OrderedDict([('likelihood.raw_task_noises', tensor([-5.2343, -6.5310, -7.4318, -7.2235])), ('likelihood.raw_task_noises_constraint.lower_bound', tensor(1.0000e-04)), ('likelihood.raw_task_noises_constraint.upper_bound', tensor(inf)), ('mean_module.base_means.0.raw_constant', tensor(0.1395)), ('mean_module.base_means.1.raw_constant', tensor(-0.1869)), ('mean_module.base_means.2.raw_constant', tensor(0.3530)), ('mean_module.base_means.3.raw_constant', tensor(-0.5176)), ('covar_module.task_covar_module.covar_factor', tensor([[-0.7902],
        [ 1.2869],
        [-0.9835],
        [ 0.8773]])), ('covar_module.task_covar_module.raw_var', tensor([ 0.2616, -2.1189, -0.3932, -0.0664])), ('covar_module.task_covar_module.raw_var_constraint.lower_bound', tensor(0.)), ('covar_module.task_covar_module.raw_var_constraint.upper_bound', tensor(inf)), ('covar_module.data_covar_module.raw_mixture_weights', tensor([-0.7583, -0.7583, -1.6596])), ('covar_module.data_covar_module.raw_mixture_means', tensor([[[-1.9053, -2.4514, -2.1470, -2.2573, -2.1732, -2.0538, -3.2200,
          -1.7760]],

        [[-1.9053, -2.4514, -2.1470, -2.2573, -2.1732, -2.0538, -3.2200,
          -1.7760]],

        [[-2.7477, -1.3673, -2.3612, -2.1956, -3.6489, -2.6292, -2.0415,
          -1.8324]]])), ('covar_module.data_covar_module.raw_mixture_scales', tensor([[[ 6.5096e-04, -2.5221e+00, -1.0185e+00, -1.4447e+00, -1.2329e+00,
          -8.4825e-01, -2.7532e+00, -1.5032e+00]],

        [[ 6.5096e-04, -2.5221e+00, -1.0185e+00, -1.4447e+00, -1.2329e+00,
          -8.4825e-01, -2.7532e+00, -1.5032e+00]],

        [[-2.2346e+00,  5.6568e+00, -1.6460e+00, -1.3241e+00, -3.2332e+00,
          -2.1581e+00,  5.3105e-01,  2.7624e-01]]])), ('covar_module.data_covar_module.raw_mixture_scales_constraint.lower_bound', tensor(0.)), ('covar_module.data_covar_module.raw_mixture_scales_constraint.upper_bound', tensor(inf)), ('covar_module.data_covar_module.raw_mixture_means_constraint.lower_bound', tensor(0.)), ('covar_module.data_covar_module.raw_mixture_means_constraint.upper_bound', tensor(inf)), ('covar_module.data_covar_module.raw_mixture_weights_constraint.lower_bound', tensor(0.)), ('covar_module.data_covar_module.raw_mixture_weights_constraint.upper_bound', tensor(inf))])
